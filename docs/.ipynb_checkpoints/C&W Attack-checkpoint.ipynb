{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b63c013-9aa9-4504-b08f-4bcf17cb703f",
   "metadata": {},
   "source": [
    "> Reference: Adversarial Attacks with Carlini & Wagner Approach: https://medium.com/@zachariaharungeorge/adversarial-attacks-with-carlini-wagner-approach-8307daa9a503"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f4594d-bc1e-4618-a79a-3e4d6409a6a5",
   "metadata": {},
   "source": [
    "# Formulating the C&W Attack as an Optimization Problem\n",
    "\n",
    "The Carlini & Wagner (C&W) attack formulates the generation of adversarial examples as an optimization problem, seeking to find the smallest perturbation to the input data that causes a misclassification by the target model. This optimization problem is crafted to balance the imperceptibility of the perturbation with the effectiveness of inducing misclassification. Let's delve deeper into the formulation of the C&W attack:\n",
    "\n",
    "## 1. Defining the Objective Function\n",
    "\n",
    "The C&W attack begins by defining an objective function, $J(x')$, that quantifies the goals of the attack.\n",
    "\n",
    "$$\n",
    "J(x') = \\alpha \\cdot \\text{dist}(x, x') + \\beta \\cdot \\text{loss}(f(x'), y_t)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $x$ is the original input.\n",
    "- $x'$ is the perturbed input.\n",
    "- $\\text{dist}(x, x')$ measures the perturbation, typically using the L2 or L∞ norm.\n",
    "- $\\text{loss}(f(x'), y_t)$ represents the misclassification loss of the target model $f$ on the perturbed input with respect to the target class $y_t$.\n",
    "- $\\alpha$ and $\\beta$ are weights that balance the two objectives.\n",
    "\n",
    "The objectives are:\n",
    "- **Minimizing the perturbation:** To ensure that the adversarial example remains visually similar to the original input.\n",
    "- **Maximizing the misclassification confidence:** To guarantee that the perturbed input is misclassified by the target model.\n",
    "\n",
    "## 2. Optimization Algorithm\n",
    "\n",
    "The C&W attack is an iterative process that refines the adversarial example through multiple iterations. The optimization algorithm adjusts the perturbation to improve the chances of misclassification while keeping the perturbation imperceptible.\n",
    "\n",
    "**Gradient Descent:** This common optimization algorithm uses the gradients of the objective function with respect to the input, adjusting the input in the opposite direction of these gradients. This process is repeated iteratively to converge towards an adversarial example.\n",
    "\n",
    "$$\n",
    "x'_n = x' - \\eta \\cdot \\nabla_{x'} J(x')\n",
    "$$\n",
    "\n",
    "where $\\eta$ is the step size, determining the magnitude of adjustments.\n",
    "\n",
    "## 3. Balancing Trade-offs\n",
    "\n",
    "**Trade-off Parameter Tuning:** The weights $\\alpha$ and $\\beta$ in the objective function determine the trade-off between minimizing perturbation and maximizing misclassification. Tuning these parameters allows for emphasis on one aspect over the other based on specific requirements of the attack.\n",
    "\n",
    "## 4. Adaptability to Threat Models\n",
    "\n",
    "The optimization problem is tailored to different threat models by considering different norms, such as the L2 norm (Euclidean distance) or the L∞ norm (maximum perturbation). This adaptability allows the C&W attack to address a variety of scenarios and evaluation criteria.\n",
    "\n",
    "For example:\n",
    "- For the L2 norm: $\\text{dist}(x, x') = \\|x - x'\\|_2$\n",
    "- For the L∞ norm: $\\text{dist}(x, x') = \\max(\\|x - x'\\|_\\infty - \\epsilon, 0)$, where $\\epsilon$ is a constraint on the maximum perturbation.\n",
    "\n",
    "## 5. Handling Model Uncertainties\n",
    "\n",
    "To counter gradient masking, where models intentionally obscure their gradients, the C&W attack incorporates strategies such as randomization during optimization. This introduces an element of uncertainty into the gradient computation process.\n",
    "\n",
    "$$\n",
    "\\nabla_{x'} J(x') = \\nabla_{x'} J(x') + \\text{random noise}\n",
    "$$\n",
    "\n",
    "Introducing random noise ensures that the gradient estimation remains resilient even when the model attempts to hide its true gradients.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
