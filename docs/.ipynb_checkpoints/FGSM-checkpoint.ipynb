{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59225dc4-8e6b-4ee4-892d-c43b424785a4",
   "metadata": {},
   "source": [
    "# Fast Gradient Sign Method (FGSM)\n",
    "\n",
    "The Fast Gradient Sign Method (FGSM) is one of the most well-known adversarial attack methods. It was introduced by Ian Goodfellow et al. in the paper [\"Explaining and Harnessing Adversarial Examples\" (2015)](https://arxiv.org/abs/1412.6572).\n",
    "\n",
    "## Intuition\n",
    "\n",
    "FGSM generates adversarial examples by leveraging the gradient of the loss function with respect to the input image. The method slightly perturbs the input in the direction that maximizes the loss, thereby causing the model to misclassify the input.\n",
    "\n",
    "## Mathematical Foundation\n",
    "\n",
    "Let:\n",
    "\n",
    "$x$: The original input (image) to the model\n",
    "\n",
    "$y$: The true label of the input\n",
    "\n",
    "$\\theta$: The parameters of the model\n",
    "\n",
    "$L(\\theta, x, y)$: The loss function (e.g., cross-entropy loss)\n",
    "\n",
    "$\\epsilon$: The perturbation magnitude (a small constant that controls the strength of the attack)\n",
    "\n",
    "The adversarial example $x_{adv}$ is computed as:\n",
    "\n",
    "## Key Steps:\n",
    "\n",
    "Compute the gradient: Compute the gradient of the loss function with respect to the input, $\\nabla_x L(\\theta, x, y)$.\n",
    "\n",
    "Take the sign of the gradient: Extract the sign of each component of the gradient.\n",
    "\n",
    "Add the perturbation: Add a scaled version of the sign of the gradient to the original input.\n",
    "\n",
    "This perturbation pushes the input slightly along the direction of the gradient that maximizes the loss.\n",
    "\n",
    "Properties:\n",
    "\n",
    "The perturbation is constrained to have a fixed $L_\\infty$ norm (bounded by $\\epsilon$).\n",
    "\n",
    "The attack is \"fast\" because it only requires a single gradient computation.\n",
    "\n",
    "\n",
    "## Algorithm\n",
    "\n",
    "Compute the gradient of the loss with respect to the input:\n",
    "\n",
    "\n",
    "Compute the adversarial perturbation:\n",
    "\n",
    "\n",
    "Add the perturbation to the original input:\n",
    "\n",
    "\n",
    "Clip the resulting adversarial example to ensure it remains a valid input (e.g., pixel values remain in [0, 1])\n",
    "\n",
    "## Advantages\n",
    "\n",
    "- Fast and Simple: Requires only a single gradient computation, making it computationally efficient.\n",
    "\n",
    "- Effective: Can significantly reduce the accuracy of a model with minimal perturbation.\n",
    "\n",
    "## Limitations\n",
    "\n",
    "- Perceptibility: Larger $\\epsilon$ values can make perturbations visually noticeable.\n",
    "\n",
    "- White-box assumption: FGSM requires access to the modelâ€™s gradients, which may not be available in black-box settings."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
