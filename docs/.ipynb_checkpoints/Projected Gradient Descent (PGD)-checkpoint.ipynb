{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac00d13f-ff5e-4617-bdd4-af87c3d1ce67",
   "metadata": {},
   "source": [
    "> Reference:\n",
    "> \n",
    "> Unveiling the Power of Projected Gradient Descent in Adversarial Attacks: https://medium.com/@zachariaharungeorge/unveiling-the-power-of-projected-gradient-descent-in-adversarial-attacks-2f92509dde3c\n",
    "> \n",
    "> Working with Results: https://docs.ultralytics.com/modes/predict/#working-with-results\n",
    "> \n",
    "> Understanding output of .pt file of YOLOv8: https://github.com/ultralytics/ultralytics/issues/8421"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd4beb6-cefe-4bc3-af6a-2190bb6a83a3",
   "metadata": {},
   "source": [
    "# Working Mechanism of Projected Gradient Descent (PGD)\n",
    "\n",
    "At the core of machine learning optimization lies the fundamental concept of gradient descent. This iterative algorithm fine-tunes model parameters to minimize a given loss function. Mathematically, the update rule is expressed as:\n",
    "\n",
    "$$\\Theta_{t+1} = \\Theta_t - \\alpha \\cdot \\nabla J(\\Theta_t)$$\n",
    "\n",
    "where $\\Theta_t$ represents the parameters at iteration $t$, $\\alpha$ is the learning rate, and $\\nabla J(\\Theta_t)$ is the gradient of the loss function.\n",
    "\n",
    "## Projected Gradient Descent (PGD)\n",
    "\n",
    "Projected Gradient Descent (PGD) builds upon this foundation, introducing thoughtful constraints to enhance its effectiveness in crafting adversarial examples. In the context of adversarial attacks, the objective is to perturb input data to deliberately mislead the model.\n",
    "\n",
    "PGD incorporates a perturbation budget ($\\epsilon$) and a step size ($\\alpha$) to control the amount and direction of perturbation. The update rule for PGD is defined as:\n",
    "\n",
    "$$x'_{t+1} = \\Pi(x_t + \\alpha \\cdot \\text{sign}(\\nabla_x J(\\Theta, x_t, y)))$$\n",
    "\n",
    "where $x_t$​ is the input at iteration $t$, $\\alpha$ is the step size, $\\nabla_x J(\\Theta, x_t, y)$ is the gradient of the loss with respect to the input, and $\\Pi$​ is the projection operator ensuring perturbed input stays within predefined bounds.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
