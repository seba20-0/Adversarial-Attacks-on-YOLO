{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a917968-f3f3-4cf4-971c-9326dbe898f7",
   "metadata": {},
   "source": [
    "> Reference: Boosting Adversarial Attacks with Momentum: https://arxiv.org/abs/1710.06081"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab377d4-5f9c-4418-bf8f-c74c45154c10",
   "metadata": {},
   "source": [
    "# Momentum Iterative Method (MIM)\n",
    "## 2. Backgrounds\n",
    "\n",
    "Adversarial attacks aim to create input samples that are misclassified by classifiers. These attacks can be non-targeted, where the perturbed input is misclassified as any incorrect label, or targeted, where it is misclassified as a specific incorrect label. The perturbation is typically small and bound by an \\(L_p\\) norm.\n",
    "\n",
    "### 2.1. Attack methods\n",
    "\n",
    "There are three main categories for generating adversarial examples:\n",
    "\n",
    "- **One-step gradient-based methods:** Methods like Fast Gradient Sign Method (FGSM) use the gradient of the loss to create adversarial examples.\n",
    "- **Iterative methods:** These methods apply the gradient method multiple times with a small step size, improving the strength of the attack at the cost of transferability.\n",
    "- **Optimization-based methods:** These methods optimize the adversarial examples directly, often using techniques like L-BFGS.\n",
    "\n",
    "### 2.2. Defense methods\n",
    "\n",
    "Adversarial training is the primary method for increasing model robustness against adversarial examples, especially through techniques like ensemble adversarial training, which uses adversarial examples generated from multiple models.\n",
    "\n",
    "## 3. Methodology\n",
    "\n",
    "The Momentum Iterative Method (MIM) introduces momentum into iterative FGSM to create adversarial examples. It combines the advantages of iterative and momentum-based methods, increasing the transferability and strength of the attacks.\n",
    "\n",
    "### 3.1. Momentum iterative fast gradient sign method\n",
    "\n",
    "This method uses a velocity vector to stabilize gradient updates over iterations, helping to escape poor local optima and improving the attack's effectiveness.\n",
    "\n",
    "**Algorithm: MI-FGSM**\n",
    "\n",
    "```plaintext\n",
    "Input: Classifier, loss function, initial example, label, perturbation size, iterations, decay factor\n",
    "Output: Adversarial example within specified \\(L_\\infty\\) norm\n",
    "1: Initialize step size and velocity vector\n",
    "2: For each iteration, compute gradient, update velocity and apply it\n",
    "3: Return final adversarial example\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
