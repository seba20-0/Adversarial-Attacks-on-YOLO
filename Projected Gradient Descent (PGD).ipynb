{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac00d13f-ff5e-4617-bdd4-af87c3d1ce67",
   "metadata": {},
   "source": [
    "> Reference:\n",
    "> \n",
    "> Unveiling the Power of Projected Gradient Descent in Adversarial Attacks: https://medium.com/@zachariaharungeorge/unveiling-the-power-of-projected-gradient-descent-in-adversarial-attacks-2f92509dde3c\n",
    "> \n",
    "> Working with Results: https://docs.ultralytics.com/modes/predict/#working-with-results\n",
    "> \n",
    "> Understanding output of .pt file of YOLOv8: https://github.com/ultralytics/ultralytics/issues/8421"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300f3ef8-16e6-4900-b0e9-4b9d38dfe969",
   "metadata": {},
   "source": [
    "# Issues\n",
    "In the `pgd_attack(model, images, labels, epsilon, alpha, num_iterations)` funtion, the outputs of `model(perturbed_images)` is a list. It's make sense since in the Ultralytics documentation`All Ultralytics predict() calls will return a list of Results objects`. However, the `CrossEntropyLoss()` here requires `argument 'input' (position 1) must be Tensor, not list`. I then check the above Github issues, seems like every pre-trained YOLOv8 model will return different result format. Not sure about YOLOv8 underlying way of handling output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd4beb6-cefe-4bc3-af6a-2190bb6a83a3",
   "metadata": {},
   "source": [
    "# Working Mechanism of Projected Gradient Descent (PGD)\n",
    "\n",
    "At the core of machine learning optimization lies the fundamental concept of gradient descent. This iterative algorithm fine-tunes model parameters to minimize a given loss function. Mathematically, the update rule is expressed as:\n",
    "\n",
    "$$\\Theta_{t+1} = \\Theta_t - \\alpha \\cdot \\nabla J(\\Theta_t)$$\n",
    "\n",
    "where $\\Theta_t$ represents the parameters at iteration $t$, $\\alpha$ is the learning rate, and $\\nabla J(\\Theta_t)$ is the gradient of the loss function.\n",
    "\n",
    "## Projected Gradient Descent (PGD)\n",
    "\n",
    "Projected Gradient Descent (PGD) builds upon this foundation, introducing thoughtful constraints to enhance its effectiveness in crafting adversarial examples. In the context of adversarial attacks, the objective is to perturb input data to deliberately mislead the model.\n",
    "\n",
    "PGD incorporates a perturbation budget ($\\epsilon$) and a step size ($\\alpha$) to control the amount and direction of perturbation. The update rule for PGD is defined as:\n",
    "\n",
    "$$x'_{t+1} = \\Pi(x_t + \\alpha \\cdot \\text{sign}(\\nabla_x J(\\Theta, x_t, y)))$$\n",
    "\n",
    "where $x_t$​ is the input at iteration $t$, $\\alpha$ is the step size, $\\nabla_x J(\\Theta, x_t, y)$ is the gradient of the loss with respect to the input, and $\\Pi$​ is the projection operator ensuring perturbed input stays within predefined bounds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962ca5c1-0c40-4d20-91ce-6b73655359f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "31e518b4-9297-4040-a902-1a091a357b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from ultralytics import YOLO\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "\n",
    "# Step 1: Load trained YOLOv8 model\n",
    "model = YOLO('train/weights/best.pt')\n",
    "image_path = 'stop.png'\n",
    "\n",
    "# Step 2: Extract the backbone (CSPDarknet53)\n",
    "backbone = model.model.model[:10]\n",
    "labels = torch.tensor([22])\n",
    "\n",
    "num_classes = 29\n",
    "sample_image = torch.randn(1, 3, 416, 416)  # Adjust size if necessary\n",
    "sample_output = backbone(sample_image)\n",
    "output_channels = sample_output.shape[1]\n",
    "classify_model = nn.Sequential(\n",
    "    backbone,  # Use the CSPDarknet53 backbone\n",
    "    nn.AdaptiveAvgPool2d((1, 1)),  # Global Average Pooling to reduce to (batch_size, channels, 1, 1)\n",
    "    nn.Flatten(),  # Flatten to (batch_size, channels)\n",
    "    nn.Linear(in_features=output_channels, out_features=num_classes)  # Linear layer for classification\n",
    ")\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((416, 416)),  # Resize to 416x416\n",
    "    transforms.ToTensor(),          # Convert to tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize\n",
    "])\n",
    "\n",
    "image = Image.open(image_path).convert(\"RGB\")  # Load image and ensure it's in RGB format\n",
    "image = preprocess(image).unsqueeze(0)  # Apply preprocessing and add batch dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313523c8-e3c0-4bcc-a214-8efebf28b1a0",
   "metadata": {},
   "source": [
    "Implement Projected Gradient Descent (PGD) here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7066abb7-539b-4f5b-b9bb-d4639b4a5309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iamges:  tensor([[[[2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "          ...,\n",
      "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489]],\n",
      "\n",
      "         [[2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "          ...,\n",
      "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286]],\n",
      "\n",
      "         [[2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "          ...,\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400]]]])\n",
      "perturbed_images:  tensor([[[[2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "          ...,\n",
      "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489]],\n",
      "\n",
      "         [[2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "          ...,\n",
      "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286]],\n",
      "\n",
      "         [[2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "          ...,\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400]]]], requires_grad=True)\n",
      "\n",
      "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
      "0: 416x416 1 Stop_Sign, 77.0ms\n",
      "Speed: 0.0ms preprocess, 77.0ms inference, 1.8ms postprocess per image at shape (1, 3, 416, 416)\n",
      "outputs:  [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: None\n",
      "masks: None\n",
      "names: {0: '-Road narrows on right', 1: '50 mph speed limit', 2: 'Attention Please-', 3: 'Beware of children', 4: 'CYCLE ROUTE AHEAD WARNING', 5: 'Dangerous Left Curve Ahead', 6: 'Dangerous Rright Curve Ahead', 7: 'End of all speed and passing limits', 8: 'Give Way', 9: 'Go Straight or Turn Right', 10: 'Go straight or turn left', 11: 'Keep-Left', 12: 'Keep-Right', 13: 'Left Zig Zag Traffic', 14: 'No Entry', 15: 'No_Over_Taking', 16: 'Overtaking by trucks is prohibited', 17: 'Pedestrian Crossing', 18: 'Round-About', 19: 'Slippery Road Ahead', 20: 'Speed Limit 20 KMPh', 21: 'Speed Limit 30 KMPh', 22: 'Stop_Sign', 23: 'Straight Ahead Only', 24: 'Traffic_signal', 25: 'Truck traffic is prohibited', 26: 'Turn left ahead', 27: 'Turn right ahead', 28: 'Uneven Road'}\n",
      "obb: None\n",
      "orig_img: array([[[2, 2, 2],\n",
      "        [2, 2, 2],\n",
      "        [2, 2, 2],\n",
      "        ...,\n",
      "        [2, 2, 2],\n",
      "        [2, 2, 2],\n",
      "        [2, 2, 2]],\n",
      "\n",
      "       [[2, 2, 2],\n",
      "        [2, 2, 2],\n",
      "        [2, 2, 2],\n",
      "        ...,\n",
      "        [2, 2, 2],\n",
      "        [2, 2, 2],\n",
      "        [2, 2, 2]],\n",
      "\n",
      "       [[2, 2, 2],\n",
      "        [2, 2, 2],\n",
      "        [2, 2, 2],\n",
      "        ...,\n",
      "        [2, 2, 2],\n",
      "        [2, 2, 2],\n",
      "        [2, 2, 2]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[2, 2, 2],\n",
      "        [2, 2, 2],\n",
      "        [2, 2, 2],\n",
      "        ...,\n",
      "        [2, 2, 2],\n",
      "        [2, 2, 2],\n",
      "        [2, 2, 2]],\n",
      "\n",
      "       [[2, 2, 2],\n",
      "        [2, 2, 2],\n",
      "        [2, 2, 2],\n",
      "        ...,\n",
      "        [2, 2, 2],\n",
      "        [2, 2, 2],\n",
      "        [2, 2, 2]],\n",
      "\n",
      "       [[2, 2, 2],\n",
      "        [2, 2, 2],\n",
      "        [2, 2, 2],\n",
      "        ...,\n",
      "        [2, 2, 2],\n",
      "        [2, 2, 2],\n",
      "        [2, 2, 2]]], dtype=uint8)\n",
      "orig_shape: (416, 416)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/detect/predict'\n",
      "speed: {'preprocess': 0.02288818359375, 'inference': 77.04830169677734, 'postprocess': 1.8184185028076172}]\n",
      "outputs[0] ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: None\n",
      "masks: None\n",
      "names: {0: '-Road narrows on right', 1: '50 mph speed limit', 2: 'Attention Please-', 3: 'Beware of children', 4: 'CYCLE ROUTE AHEAD WARNING', 5: 'Dangerous Left Curve Ahead', 6: 'Dangerous Rright Curve Ahead', 7: 'End of all speed and passing limits', 8: 'Give Way', 9: 'Go Straight or Turn Right', 10: 'Go straight or turn left', 11: 'Keep-Left', 12: 'Keep-Right', 13: 'Left Zig Zag Traffic', 14: 'No Entry', 15: 'No_Over_Taking', 16: 'Overtaking by trucks is prohibited', 17: 'Pedestrian Crossing', 18: 'Round-About', 19: 'Slippery Road Ahead', 20: 'Speed Limit 20 KMPh', 21: 'Speed Limit 30 KMPh', 22: 'Stop_Sign', 23: 'Straight Ahead Only', 24: 'Traffic_signal', 25: 'Truck traffic is prohibited', 26: 'Turn left ahead', 27: 'Turn right ahead', 28: 'Uneven Road'}\n",
      "obb: None\n",
      "orig_img: array([[[2, 2, 2],\n",
      "        [2, 2, 2],\n",
      "        [2, 2, 2],\n",
      "        ...,\n",
      "        [2, 2, 2],\n",
      "        [2, 2, 2],\n",
      "        [2, 2, 2]],\n",
      "\n",
      "       [[2, 2, 2],\n",
      "        [2, 2, 2],\n",
      "        [2, 2, 2],\n",
      "        ...,\n",
      "        [2, 2, 2],\n",
      "        [2, 2, 2],\n",
      "        [2, 2, 2]],\n",
      "\n",
      "       [[2, 2, 2],\n",
      "        [2, 2, 2],\n",
      "        [2, 2, 2],\n",
      "        ...,\n",
      "        [2, 2, 2],\n",
      "        [2, 2, 2],\n",
      "        [2, 2, 2]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[2, 2, 2],\n",
      "        [2, 2, 2],\n",
      "        [2, 2, 2],\n",
      "        ...,\n",
      "        [2, 2, 2],\n",
      "        [2, 2, 2],\n",
      "        [2, 2, 2]],\n",
      "\n",
      "       [[2, 2, 2],\n",
      "        [2, 2, 2],\n",
      "        [2, 2, 2],\n",
      "        ...,\n",
      "        [2, 2, 2],\n",
      "        [2, 2, 2],\n",
      "        [2, 2, 2]],\n",
      "\n",
      "       [[2, 2, 2],\n",
      "        [2, 2, 2],\n",
      "        [2, 2, 2],\n",
      "        ...,\n",
      "        [2, 2, 2],\n",
      "        [2, 2, 2],\n",
      "        [2, 2, 2]]], dtype=uint8)\n",
      "orig_shape: (416, 416)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/detect/predict'\n",
      "speed: {'preprocess': 0.02288818359375, 'inference': 77.04830169677734, 'postprocess': 1.8184185028076172}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cross_entropy_loss(): argument 'input' (position 1) must be Tensor, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 48\u001b[0m\n\u001b[1;32m     45\u001b[0m num_iterations \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m  \u001b[38;5;66;03m# Number of iterations\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Applying the PGD attack on the input image\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m pgd_perturbed_image \u001b[38;5;241m=\u001b[39m \u001b[43mpgd_attack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_iterations\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Clamping the perturbed image to ensure pixel values are within valid range\u001b[39;00m\n\u001b[1;32m     51\u001b[0m pgd_perturbed_image \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mclamp(pgd_perturbed_image, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[55], line 19\u001b[0m, in \u001b[0;36mpgd_attack\u001b[0;34m(model, images, labels, epsilon, alpha, num_iterations)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# TypeError: cross_entropy_loss(): argument 'input' (position 1) must be Tensor, not list\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs[0]\u001b[39m\u001b[38;5;124m\"\u001b[39m, outputs[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m---> 19\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCrossEntropyLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Zero all existing gradients\u001b[39;00m\n\u001b[1;32m     22\u001b[0m model\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/.virtualenvs/mlr503/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/mlr503/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.virtualenvs/mlr503/lib/python3.12/site-packages/torch/nn/modules/loss.py:1293\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1292\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1294\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1297\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1298\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1299\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1300\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/mlr503/lib/python3.12/site-packages/torch/nn/functional.py:3479\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3478\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3479\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3480\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3481\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3482\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3483\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3484\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3485\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3486\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: cross_entropy_loss(): argument 'input' (position 1) must be Tensor, not list"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Projected Gradient Descent (PGD) Attack\n",
    "# This function implements the PGD attack, similar in style to the existing FGSM function.\n",
    "def pgd_attack(model, images, labels, epsilon, alpha, num_iterations):\n",
    "    print(\"iamges: \", images) # tensor\n",
    "    # Make a copy of the original images to perturb\n",
    "    perturbed_images = images.clone().detach()\n",
    "    perturbed_images.requires_grad = True\n",
    "    print(\"perturbed_images: \", perturbed_images) # tensor\n",
    "    \n",
    "    for _ in range(num_iterations):\n",
    "        # Forward pass to compute loss\n",
    "        outputs = model(perturbed_images)\n",
    "        print(\"outputs: \", outputs) # ultralytics.engine.results.Results object\n",
    "        # TypeError: cross_entropy_loss(): argument 'input' (position 1) must be Tensor, not list\n",
    "        print(\"outputs[0]\", outputs[0])\n",
    "        loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "\n",
    "        # Zero all existing gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Calculate gradients of the loss with respect to the images\n",
    "        loss.backward()\n",
    "\n",
    "        # Compute the perturbation (gradient ascent)\n",
    "        with torch.no_grad():\n",
    "            perturbation = alpha * perturbed_images.grad.sign()\n",
    "            perturbed_images += perturbation\n",
    "\n",
    "            # Project perturbed images to ensure they remain within the epsilon-ball of the original images\n",
    "            perturbation_clipped = torch.clamp(perturbed_images - images, min=-epsilon, max=epsilon)\n",
    "            perturbed_images = torch.clamp(images + perturbation_clipped, 0, 1)\n",
    "\n",
    "        # Reset the gradients for the next iteration\n",
    "        perturbed_images.grad.zero_()\n",
    "    \n",
    "    return perturbed_images\n",
    "\n",
    "# Example usage of the PGD attack\n",
    "# Assuming you have already loaded the YOLOv8 model and input image tensors\n",
    "alpha = 0.01  # Step size for each iteration\n",
    "epsilon = 0.1  # Perturbation budget\n",
    "num_iterations = 10  # Number of iterations\n",
    "\n",
    "# Applying the PGD attack on the input image\n",
    "pgd_perturbed_image = pgd_attack(model, image, labels, epsilon, alpha, num_iterations)\n",
    "\n",
    "# Clamping the perturbed image to ensure pixel values are within valid range\n",
    "pgd_perturbed_image = torch.clamp(pgd_perturbed_image, 0, 1)\n",
    "\n",
    "# Getting predictions for the original and perturbed images\n",
    "boxes, scores, labels = get_yolo_output(model, image)\n",
    "pgd_boxes, pgd_scores, pgd_labels = get_yolo_output(model, pgd_perturbed_image)\n",
    "\n",
    "# Visualization\n",
    "fig, axs = plt.subplots(1, 3, figsize=(20, 10))\n",
    "\n",
    "# Original image with predictions\n",
    "plot_boxes(axs[0], boxes, scores, labels, \"Original Image with YOLOv8 Predictions\", image)\n",
    "\n",
    "# Perturbation from PGD\n",
    "pgd_perturbation = (pgd_perturbed_image - image).squeeze().permute(1, 2, 0).cpu().detach().numpy()\n",
    "pgd_perturbation = (pgd_perturbation - pgd_perturbation.min()) / (pgd_perturbation.max() - pgd_perturbation.min())\n",
    "axs[1].imshow(pgd_perturbation)\n",
    "axs[1].set_title(\"PGD Perturbation\")\n",
    "axs[1].axis('off')\n",
    "\n",
    "# Adversarial image with predictions\n",
    "plot_boxes(axs[2], pgd_boxes, pgd_scores, pgd_labels, \"Adversarial Image with YOLOv8 Predictions\", pgd_perturbed_image)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829800b5-4d54-43ce-8b51-cdc17cc40f61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
